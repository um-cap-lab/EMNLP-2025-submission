{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# Minicons Installation\n",
    "# Introduction can be found https://kanishka.xyz/post/minicons-running-large-scale-behavioral-analyses-on-transformer-lms/\n",
    "# Tutorial and code can be found https://github.com/kanishkamisra/minicons/blob/master/examples/surprisals.md\n",
    "#!pip install minicons\n",
    "\n",
    "from minicons import scorer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Resizing Model Embeddings (50527) to Match with Tokenizer Vocabulary Size (50528)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# uncomment block if needed\n",
    "'''\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"gpt2-small/checkpoint-trainedtokenizer_100M\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# print mismatch\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "print(\"Model vocab size:\", model.config.vocab_size)\n",
    "\n",
    "# resize model embeddings to match tokenizer\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    print(f\"Resizing model embeddings from {model.config.vocab_size} → {len(tokenizer)}\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.save_pretrained(model_path)\n",
    "    print(\"Saved updated model.\")\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizer vocab size: 50258\n",
      "Model vocab size: 50257\n",
      "Resizing model embeddings from 50257 → 50258\n",
      "Saved updated model.\n",
      "Special tokens map: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "model_path = \"gpt2-small/checkpoint-trainedtokenizer_100M\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# wrap with minicons scorer\n",
    "lm_scorer = scorer.IncrementalLMScorer(model_path, device = \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"Special token IDs:\", tokenizer.all_special_ids)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Special tokens: ['<|endoftext|>']\n",
      "Special token IDs: [50257]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print(\"Special tokens map:\", tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Special tokens map: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "surprisal"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('<pad>', 0.0),\n",
       "  ('ĠTheĠ', 4.693825721740723),\n",
       "  ('balloon', 9.007822036743164),\n",
       "  ('Ġwa', 2.1047677993774414),\n",
       "  ('s', 0.002323150634765625),\n",
       "  ('Ġinf', 10.309831619262695),\n",
       "  ('lat', 2.666797637939453),\n",
       "  ('ingĠfor', 6.27564811706543),\n",
       "  ('Ġ10', 7.321373462677002),\n",
       "  ('Ġminute', 2.1019039154052734),\n",
       "  ('s', 0.0007076263427734375)],\n",
       " [('<pad>', 0.0),\n",
       "  ('Ġpl', 7.494743824005127),\n",
       "  ('ace', 5.744235992431641),\n",
       "  ('holder', 11.565694808959961)]]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# takes in a sentence, and output the surprisal values for each word\n",
    "\n",
    "def calculate_surprisal(sentence):\n",
    "    input_sentence = sentence # process per sentence, never in batches to avoid padding\n",
    "    # token_score() function of Minicons takes in several parameters\n",
    "    # if surprisal = True, the output value is surprisal instead of log likelihood\n",
    "    # if base_two = True, the log likelihood will be in base 2\n",
    "    # see Minicons documentations for details\n",
    "    # score tokens\n",
    "    token_surprisals = lm_scorer.token_score(input_sentence, surprisal = True, base_two = False)[0]\n",
    "\n",
    "    # tokenizer setup\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping = True, add_special_tokens = False)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    token_ids = encoding['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "    # filter out special token surprisals (like <pad>) *not needed if we input sentences one-by-one, but just to be safe\n",
    "    special_tokens = set(tokenizer.all_special_tokens + ['<pad>'])\n",
    "    filtered = [\n",
    "        (token, score, span)\n",
    "        for (token, score), span in zip(token_surprisals, offsets)\n",
    "        if token not in special_tokens\n",
    "    ]\n",
    "\n",
    "    # prepare: group surprisals by words based on character spans\n",
    "    words = re.findall(r\"\\S+\", sentence)\n",
    "    word_spans = []\n",
    "    i = 0\n",
    "    for match in re.finditer(r\"\\S+\", sentence):\n",
    "        start, end = match.span()\n",
    "        word_spans.append((i, start, end))\n",
    "        i += 1\n",
    "\n",
    "    # assign tokens to words based on character alignment (needed since BPE tokenizers break words down into subwords/tokens)\n",
    "    word_surprisals = []\n",
    "    word_index = 0\n",
    "    word_start, word_end = word_spans[word_index][1:3] # previously: word_spans.append((i, start, end)) [0, 1, 2]\n",
    "    current_surprisal = 0.0\n",
    "\n",
    "    for token, score, (start, end) in filtered:\n",
    "        if start >= word_end:\n",
    "            word_surprisals.append((words[word_index], current_surprisal))\n",
    "            word_index += 1\n",
    "            if word_index >= len(word_spans):\n",
    "                break\n",
    "            word_start, word_end = word_spans[word_index][1:3]\n",
    "            current_surprisal = 0.0\n",
    "        current_surprisal += score\n",
    "\n",
    "    # append final word\n",
    "    if word_index < len(words):\n",
    "        word_surprisals.append((words[word_index], current_surprisal))\n",
    "\n",
    "    return word_surprisals\n",
    "\n",
    "sentence = 'The balloon was inflating for 10 minutes'\n",
    "calculate_surprisal(sentence)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 0.0),\n",
       " ('balloon', 4.693825721740723),\n",
       " ('was', 11.112587928771973),\n",
       " ('inflating', 12.978955268859863),\n",
       " ('for', 6.275647163391113),\n",
       " ('10', 7.32137393951416),\n",
       " ('minutes', 2.101907730102539)]"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}