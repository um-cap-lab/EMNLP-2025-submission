{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5468eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minicons Installation\n",
    "# Introduction can be found https://kanishka.xyz/post/minicons-running-large-scale-behavioral-analyses-on-transformer-lms/\n",
    "# Tutorial and code can be found https://github.com/kanishkamisra/minicons/blob/master/examples/surprisals.md\n",
    "#!pip install minicons\n",
    "\n",
    "from minicons import scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44016a1",
   "metadata": {},
   "source": [
    "#### Resizing Model Embeddings (50527) to Match with Tokenizer Vocabulary Size (50528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f349c22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50258\n",
      "Model vocab size: 50257\n",
      "Resizing model embeddings from 50257 → 50258\n",
      "Saved updated model.\n",
      "Special tokens map: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"gpt2-small/checkpoint-trainedtokenizer_100M\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# print mismatch\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "print(\"Model vocab size:\", model.config.vocab_size)\n",
    "\n",
    "# resize model embeddings to match tokenizer\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    print(f\"Resizing model embeddings from {model.config.vocab_size} → {len(tokenizer)}\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.save_pretrained(model_path)\n",
    "    print(\"Saved updated model.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96f0c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"gpt2-small/checkpoint-trainedtokenizer_100M\"\n",
    "model_path = \"gpt2-small/checkpoint-pretrainedtokenizer_100M\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# wrap with minicons scorer\n",
    "lm_scorer = scorer.IncrementalLMScorer(model_path, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfa4ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: ['<|endoftext|>']\n",
      "Special token IDs: [50257]\n",
      "Special tokens map: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"Special token IDs:\", tokenizer.all_special_ids)\n",
    "print(\"Special tokens map:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b6880e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<pad>', 0.0),\n",
       "  ('ĠTheĠ', 6.771759033203125),\n",
       "  ('balloon', 12.995538711547852),\n",
       "  ('Ġwa', 3.036536693572998),\n",
       "  ('s', 0.0033515978138893843),\n",
       "  ('Ġinf', 14.873947143554688),\n",
       "  ('lat', 3.8473756313323975),\n",
       "  ('ingĠfor', 9.053845405578613),\n",
       "  ('Ġ10', 10.562509536743164),\n",
       "  ('Ġminute', 3.032411813735962),\n",
       "  ('s', 0.0010208890307694674)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprisals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bb8b566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 0.0),\n",
       " ('teacher', 11.844358444213867),\n",
       " ('realized', 13.042094230651855),\n",
       " ('what', 6.120745658874512),\n",
       " ('the', 3.2175395488739014),\n",
       " ('storm', 15.96143913269043),\n",
       " ('rolled', 12.999858856201172),\n",
       " ('in', 5.186878204345703),\n",
       " ('while', 10.483074188232422),\n",
       " ('the', 1.5718594789505005),\n",
       " ('student', 10.762317657470703),\n",
       " ('in', 7.2503581047058105),\n",
       " ('the', 1.2104510068893433),\n",
       " ('first', 7.770709991455078),\n",
       " ('year', 5.073796272277832),\n",
       " ('was', 1.570701003074646),\n",
       " ('studying', 10.941052436828613),\n",
       " ('for', 5.377425670623779),\n",
       " ('the', 1.5096884965896606),\n",
       " ('test', 7.936253547668457),\n",
       " ('with', 9.095281600952148),\n",
       " ('great', 9.63071346282959),\n",
       " ('enthusiasm', 6.821305751800537)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def calculate_surprisal(sentence):\n",
    "    '''\n",
    "    #Takes in a sentence, and outputs surprisal values for each word.\n",
    "    '''\n",
    "    input_sentence = sentence # process per sentence, never in batches to avoid padding\n",
    "    # token_score() function of Minicons takes in several parameters\n",
    "    # if surprisal = True, the output value is surprisal instead of log likelihood\n",
    "    # if base_two = True, the log likelihood will be in base 2\n",
    "    # see Minicons documentations for details\n",
    "    # score tokens\n",
    "    token_surprisals = lm_scorer.token_score(input_sentence, surprisal = True, base_two = True)[0]\n",
    "    #print(token_surprisals)\n",
    "\n",
    "    # matching tokens manually back to words using offset mapping\n",
    "    # tokenizer setup\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping = True, add_special_tokens = False)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    token_ids = encoding['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "    # filter out special token surprisals (like <pad>) *not needed if we set add_special_tokens to False, but just to be safe\n",
    "    special_tokens = set(tokenizer.all_special_tokens + ['<pad>'])\n",
    "    filtered = [\n",
    "        (token, score, span)\n",
    "        for (token, score), span in zip(token_surprisals, offsets)\n",
    "        if token not in special_tokens\n",
    "    ]\n",
    "\n",
    "    # prepare: group surprisals by words based on character spans\n",
    "    words = re.findall(r\"\\S+\", sentence)\n",
    "    word_spans = []\n",
    "    i = 0\n",
    "    for match in re.finditer(r\"\\S+\", sentence):\n",
    "        start, end = match.span()\n",
    "        word_spans.append((i, start, end))\n",
    "        i += 1\n",
    "\n",
    "    # assign tokens to words based on character alignment (needed since BPE tokenizers break words down into subwords/tokens)\n",
    "    word_surprisals = []\n",
    "    word_index = 0\n",
    "    word_start, word_end = word_spans[word_index][1:3] # previously: word_spans.append((i, start, end)) [0, 1, 2]\n",
    "    current_surprisal = 0.0\n",
    "    \n",
    "    for token, score, (start, end) in filtered:\n",
    "        if start >= word_end:\n",
    "            word_surprisals.append((words[word_index], current_surprisal))\n",
    "            word_index += 1\n",
    "            if word_index >= len(word_spans):\n",
    "                break\n",
    "            word_start, word_end = word_spans[word_index][1:3]\n",
    "            current_surprisal = 0.0\n",
    "        current_surprisal += score\n",
    "\n",
    "    # append final word\n",
    "    if word_index < len(words):\n",
    "        word_surprisals.append((words[word_index], current_surprisal))\n",
    "\n",
    "    return word_surprisals\n",
    "\n",
    "\n",
    "sentence = 'The teacher realized what the storm rolled in while the student in the first year was studying for the test with great enthusiasm'\n",
    "calculate_surprisal(sentence)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f7fc01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 0.0),\n",
       " ('know', 4.17913293838501),\n",
       " ('that', 4.109272480010986),\n",
       " ('your', 8.175153732299805),\n",
       " ('friend', 6.524691581726074),\n",
       " ('gave', 9.396605491638184),\n",
       " ('a', 5.547191619873047),\n",
       " ('baguette', 41.23404598236084),\n",
       " ('to', 5.974528789520264),\n",
       " ('Mary', 11.711277961730957),\n",
       " ('last', 10.698275566101074),\n",
       " ('weekend', 8.592245101928711)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_surprisal(sentence):\n",
    "    '''\n",
    "    Takes in a sentence, and outputs surprisal values for each word.\n",
    "    '''\n",
    "    input_sentence = sentence\n",
    "    # token_score() function of Minicons takes in several parameters\n",
    "    # if surprisal = True, the output value is surprisal instead of log likelihood\n",
    "    # if base_two = True, the log likelihood will be in base 2\n",
    "    # see Minicons documentations for details\n",
    "    # score tokens\n",
    "    token_surprisals = lm_scorer.token_score(input_sentence, surprisal = True, base_two = True)[0]\n",
    "    #print(token_surprisals)\n",
    "\n",
    "    # filter out special tokens (like <pad>)\n",
    "    special_tokens = set(tokenizer.all_special_tokens + ['<pad>'])\n",
    "    filtered = [\n",
    "        (token, score)\n",
    "        for (token, score) in token_surprisals\n",
    "        if token not in special_tokens\n",
    "    ]\n",
    "\n",
    "   # expand tokens that contain multiple words\n",
    "    expanded = []\n",
    "    for token, score in filtered:\n",
    "        token = token.strip('Ġ') # remove space marker\n",
    "        if token.count('Ġ') > 0:\n",
    "            # multiple words inside\n",
    "            words = token.split('Ġ')\n",
    "            words = [word for word in words if word]  # remove empty strings\n",
    "            for i, word in enumerate(words):\n",
    "                expanded_token = word\n",
    "                expanded.append((expanded_token, score / len(words)))  # split surprisal equally\n",
    "        else:\n",
    "            expanded.append((token, score))    \n",
    "    #print(expanded)\n",
    "\n",
    "    words = sentence.split()\n",
    "    results = []\n",
    "\n",
    "    token_pointer = 0\n",
    "\n",
    "    for word in words:\n",
    "        accumulated = ''\n",
    "        word_surprisal = 0.0\n",
    "\n",
    "        while token_pointer < len(expanded):\n",
    "            token, surprisal = expanded[token_pointer]\n",
    "            accumulated += token\n",
    "            word_surprisal += surprisal\n",
    "            token_pointer += 1\n",
    "\n",
    "            if accumulated == word:\n",
    "                results.append((word, word_surprisal))\n",
    "                break\n",
    "        else:\n",
    "            results.append((word, word_surprisal))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "sentence = 'I know that your friend gave a baguette to Mary last weekend'\n",
    "calculate_surprisal(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_surprisal(word_surprisals):\n",
    "    \"\"\"Returns total surprisal and average surprisal per word.\"\"\"\n",
    "    scores = [score for word, score in word_surprisals]\n",
    "    total = sum(scores)\n",
    "    avg = total / len(scores)\n",
    "    return total, avg\n",
    "\n",
    "\n",
    "def compute_wh_licensing_interaction(sentences):\n",
    "    \"\"\"\n",
    "    sentences: dict with keys\n",
    "        'fg' = +Filler, +Gap\n",
    "        'fng' = +Filler, −Gap\n",
    "        'nfg' = −Filler, +Gap\n",
    "        'nfng' = −Filler, −Gap\n",
    "    Each value is a sentence string.\n",
    "\n",
    "    Returns:\n",
    "        A dict with total surprisal per sentence,\n",
    "        average surprisals per sentence,\n",
    "        and the wh-licensing interaction score.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for key, sentence in sentences.items():\n",
    "        word_surprisals = calculate_surprisal(sentence)\n",
    "        total, avg = calculate_sentence_surprisal(word_surprisals)\n",
    "        scores[key] = {'total': total, 'avg': avg, 'details': word_surprisals}\n",
    "\n",
    "    # compute wh-licensing interaction\n",
    "    interaction = (\n",
    "        (scores['fng']['total'] - scores['nfng']['total']) -\n",
    "        (scores['fg']['total'] - scores['nfg']['total'])\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'scores': scores,\n",
    "        'interaction': interaction\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b98dd7",
   "metadata": {},
   "source": [
    "#### Statistical Analysis: Mixed-Effects Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfdf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns: set_id (indicates sentence set), wh_licensor (0/1), gap (0/1), island_presence (0 = non-island, 1 = island), island_type, surprisal\n",
    "sentence_df = pd.read_csv('test_construction.csv')\n",
    "sentence_df['wh_licensor'] = sentence_df['wh_licensor'].astype('category')\n",
    "sentence_df['gap'] = sentence_df['gap'].astype('category')\n",
    "sentence_df['island_presence'] = sentence_df['island_presence'].astype('category')\n",
    "sentence_df['island_type'] = sentence_df['island_type'].astype('category')\n",
    "\n",
    "# list to store results\n",
    "interaction_results = []\n",
    "\n",
    "# a region filtering step is probably needed here\n",
    "region_df = sentence_df[sentence_df['region'] == '']\n",
    "\n",
    "def mixed_effects_linear_regression(df, label):\n",
    "    '''\n",
    "    Fits mixed-effects model and extracts wh-licensing interaction.\n",
    "    '''\n",
    "    model = smf.mixedlm(\n",
    "        \"surprisal ~ wh_licensor * gap * island_type\",\n",
    "        df,\n",
    "        groups = df[\"set_id\"],\n",
    "        re_formula = \"~wh_licensor + gap + island_type\"\n",
    ")\n",
    "\n",
    "    result = model.fit()\n",
    "    interaction_coef = result.params.get('wh_licensor[T.1]:gap[T.1]', None)\n",
    "    \n",
    "    print(f\"\\n=== {label.upper()} ===\")\n",
    "    print(result.summary())\n",
    "    \n",
    "    return interaction_coef\n",
    "\n",
    "\n",
    "interaction = mixed_effects_linear_regression(region_df, \"construction_type\") # label name to be changed according to construction type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
