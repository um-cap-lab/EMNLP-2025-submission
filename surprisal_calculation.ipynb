{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# Minicons Installation\n",
    "# Introduction can be found https://kanishka.xyz/post/minicons-running-large-scale-behavioral-analyses-on-transformer-lms/\n",
    "# Tutorial and code can be found https://github.com/kanishkamisra/minicons/blob/master/examples/surprisals.md\n",
    "#!pip install minicons\n",
    "\n",
    "from minicons import scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Resizing Model Embeddings (50527) to Match with Tokenizer Vocabulary Size (50528)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "'''\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"gpt2-small/checkpoint-trainedtokenizer_100M\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# print mismatch\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "print(\"Model vocab size:\", model.config.vocab_size)\n",
    "\n",
    "# resize model embeddings to match tokenizer\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    print(f\"Resizing model embeddings from {model.config.vocab_size} → {len(tokenizer)}\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.save_pretrained(model_path)\n",
    "    print(\"Saved updated model.\")\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizer vocab size: 50258\n",
      "Model vocab size: 50257\n",
      "Resizing model embeddings from 50257 → 50258\n",
      "Saved updated model.\n",
      "Special tokens map: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "model_path = \"gpt2-small/checkpoint-trainedtokenizer_100M\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# wrap with minicons scorer\n",
    "lm_scorer = scorer.IncrementalLMScorer(model_path, device = \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"Special token IDs:\", tokenizer.all_special_ids)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Special tokens: ['<|endoftext|>']\n",
      "Special token IDs: [50257]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print(\"Special tokens map:\", tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Special tokens map: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "surprisal"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('<pad>', 0.0),\n",
       "  ('ĠTheĠ', 4.693825721740723),\n",
       "  ('balloon', 9.007822036743164),\n",
       "  ('Ġwa', 2.1047677993774414),\n",
       "  ('s', 0.002323150634765625),\n",
       "  ('Ġinf', 10.309831619262695),\n",
       "  ('lat', 2.666797637939453),\n",
       "  ('ingĠfor', 6.27564811706543),\n",
       "  ('Ġ10', 7.321373462677002),\n",
       "  ('Ġminute', 2.1019039154052734),\n",
       "  ('s', 0.0007076263427734375)],\n",
       " [('<pad>', 0.0),\n",
       "  ('Ġpl', 7.494743824005127),\n",
       "  ('ace', 5.744235992431641),\n",
       "  ('holder', 11.565694808959961)]]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# takes in a sentence, and output the surprisal values for each word\n",
    "\n",
    "def calculate_surprisal(sentence):\n",
    "    input_sentence = sentence # process per sentence, never in batches to avoid padding\n",
    "    # token_score() function of Minicons takes in several parameters\n",
    "    # if surprisal = True, the output value is surprisal instead of log likelihood\n",
    "    # if base_two = True, the log likelihood will be in base 2\n",
    "    # see Minicons documentations for details\n",
    "    # score tokens\n",
    "    token_surprisals = lm_scorer.token_score(input_sentence, surprisal = True, base_two = True)[0]\n",
    "\n",
    "    # tokenizer setup\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping = True, add_special_tokens = False)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    token_ids = encoding['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "    # filter out special token surprisals (like <pad>) *not needed if we input sentences one-by-one, but just to be safe\n",
    "    special_tokens = set(tokenizer.all_special_tokens + ['<pad>'])\n",
    "    filtered = [\n",
    "        (token, score, span)\n",
    "        for (token, score), span in zip(token_surprisals, offsets)\n",
    "        if token not in special_tokens\n",
    "    ]\n",
    "\n",
    "    # prepare: group surprisals by words based on character spans\n",
    "    words = re.findall(r\"\\S+\", sentence)\n",
    "    word_spans = []\n",
    "    i = 0\n",
    "    for match in re.finditer(r\"\\S+\", sentence):\n",
    "        start, end = match.span()\n",
    "        word_spans.append((i, start, end))\n",
    "        i += 1\n",
    "\n",
    "    # assign tokens to words based on character alignment (needed since BPE tokenizers break words down into subwords/tokens)\n",
    "    word_surprisals = []\n",
    "    word_index = 0\n",
    "    word_start, word_end = word_spans[word_index][1:3] # previously: word_spans.append((i, start, end)) [0, 1, 2]\n",
    "    current_surprisal = 0.0\n",
    "\n",
    "    for token, score, (start, end) in filtered:\n",
    "        if start >= word_end:\n",
    "            word_surprisals.append((words[word_index], current_surprisal))\n",
    "            word_index += 1\n",
    "            if word_index >= len(word_spans):\n",
    "                break\n",
    "            word_start, word_end = word_spans[word_index][1:3]\n",
    "            current_surprisal = 0.0\n",
    "        current_surprisal += score\n",
    "\n",
    "    # append final word\n",
    "    if word_index < len(words):\n",
    "        word_surprisals.append((words[word_index], current_surprisal))\n",
    "\n",
    "    return word_surprisals\n",
    "\n",
    "sentence = 'The balloon was inflating for 10 minutes'\n",
    "calculate_surprisal(sentence)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 0.0),\n",
       " ('balloon', 4.693825721740723),\n",
       " ('was', 11.112587928771973),\n",
       " ('inflating', 12.978955268859863),\n",
       " ('for', 6.275647163391113),\n",
       " ('10', 7.32137393951416),\n",
       " ('minutes', 2.101907730102539)]"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statistical Analysis: Mixed-Effects Linear Regression Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# columns: item_id (indicating sentence set), wh_licensor (0/1), gap (0/1), island_presence (0 = non-island, 1 = island), island_type, surprisal\n",
    "sentence_df = pd.read_csv('test_sentences.csv')\n",
    "sentence_df['wh_licensor'] = sentence_df['wh_licensor'].astype('category')\n",
    "sentence_df['gap'] = sentence_df['gap'].astype('category')\n",
    "sentence_df['island_presence'] = sentence_df['island_presence'].astype('category')\n",
    "sentence_df['island_type'] = sentence_df['island_type'].astype('category')\n",
    "\n",
    "# get the list of unique island types (excluding non-islands denoted by 'None')\n",
    "island_types = [island for island in data['island_type'].unique() if it != 'None']\n",
    "\n",
    "# list to store results\n",
    "interaction_results = []\n",
    "\n",
    "def mixed_effects_linear_regression(subset, label):\n",
    "    '''\n",
    "    Fits mixed-effects model and extracts wh-licensing interaction.\n",
    "    '''\n",
    "    model = smf.mixedlm(\n",
    "        \"surprisal ~ wh_licensor * gap\",\n",
    "        subset,\n",
    "        groups = subset[\"item_id\"]\n",
    "    )\n",
    "    result = model.fit()\n",
    "    interaction_coef = result.params.get('wh_licensor[T.1]:gap[T.1]', None)\n",
    "    \n",
    "    print(f\"\\n=== {label.upper()} ===\")\n",
    "    print(result.summary())\n",
    "    \n",
    "    return interaction_coef\n",
    "\n",
    "# loop through each island type\n",
    "for island in island_types:\n",
    "    # subset data for this island type + corresponding non-island\n",
    "    subset = sentence_df[(sentence_df['island_type'] == island)]\n",
    "    \n",
    "    # check if enough data\n",
    "    if subset.shape[0] < 8:\n",
    "        print(f\"Not enough data points for {island}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # separate non-island and island subsets within island type\n",
    "    non_island_subset = subset[subset['island_presence'] == 0]\n",
    "    island_subset = subset[subset['island_presence'] == 1]\n",
    "    \n",
    "    # calculate wh-licensing interaction terms separately\n",
    "    interaction_non_island = mixed_effects_linear_regression(non_island_subset, f\"{island} (non-island)\")\n",
    "    interaction_island = mixed_effects_linear_regression(island_subset, f\"{island} (island)\")\n",
    "\n",
    "    # save results\n",
    "    interaction_results.append({\n",
    "        \"Island Type\": island,\n",
    "        \"Condition\": \"non-island\",\n",
    "        \"Wh-Licensing Interaction\": interaction_non_island\n",
    "    })\n",
    "    interaction_results.append({\n",
    "        \"Island Type\": island,\n",
    "        \"Condition\": \"island\",\n",
    "        \"Wh-Licensing Interaction\": interaction_island\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(interaction_results)\n",
    "results_df.to_csv('wh_licensing_interactions_by_island_type.csv', index = False)\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.barplot(\n",
    "    data = results_df,\n",
    "    x = \"Island Type\",\n",
    "    y = \"Wh-Licensing Interaction\",\n",
    "    hue = \"Condition\",\n",
    "    palette = \"muted\"\n",
    ")\n",
    "plt.axhline(0, color = \"black\", linestyle = \"--\")\n",
    "plt.title(\"Wh-Licensing Interaction Across Different Island Types\")\n",
    "plt.ylabel(\"Interaction Size\")\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend(title = \"Sentence Condition\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
